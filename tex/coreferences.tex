
\documentclass[a4paper,11pt]{article}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{abstract}
\usepackage{pdfpages} %to include pdfs in the file
\usepackage{booktabs,threeparttable} %for comments to tables
\let\tnote\relax % https://tex.stackexchange.com/questions/51131/using-ctable-package-with-apa6-class?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
\usepackage{geometry}
\hyphenpenalty=6000
%minimizes hyphenation
\usepackage{amsfonts} %for nice mathematical expressions
%\usepackage{acronym}
%package for table of acronyms
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{chngcntr}
%\counterwithout{figure}{section}
\usepackage{lscape}
\usepackage{longtable}
%package for tables of more than 1 page
\usepackage{tabularx}
\usepackage{array}
%packages for fixing rows of tables and centerin their content
\usepackage{rotating}
%for turning text in tables
\usepackage{verbatim}
\usepackage{setspace}
\onehalfspacing
%sets 1.5 times spaces between rows
%alternative: doublespacing
\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
%\usepackage[american]{babel}
%\usepackage{csquotes}
\usepackage{floatrow} %for having 2 graphs side by side
\usepackage{textcomp} % https://tex.stackexchange.com/questions/194083/latex-error-command-textcurrency-unavailable-in-encoding-t1
%standard code for In- and Output of signs
%Careful: american is needed for the APA citation standard
%\bibliography{biblio}
%\usepackage[style=numeric, backend=biber]{biblatex}
%\bibliography{biblio}
%\DeclareLanguageMapping{english}{american-apa}
%\usepackage[style=apa, backend=bibtex]{biblatex}
\usepackage[longnamesfirst,authoryear,sort,comma]{natbib}
\bibliographystyle{ecta}

\usepackage{graphicx}
\usepackage[permil]{overpic}
%Packages for graphics: graphicx for inserting the graphic and overpic to caption it
\usepackage{multirow}
%\usepackage{booktabs}
%Packages for tables: multirow for command \multirow, booktabs for refinements
\usepackage{amsmath, amsthm, amssymb} %Package for formulas, theorems etc.
\newtheorem{prop}{Proposition} % define prop to make propositions. http://tex.stackexchange.com/questions/125981/proposition-command-doesnt-work
  \numberwithin{equation}{section} %for equation numbering according to section
% \usepackage{accents} % for underbar
\usepackage{savefnmark}
%Package for saving footnotes and re-using them
\usepackage{paralist}
%Package for special lists
% To include two graphs in the same line
\usepackage{caption} % https://tex.stackexchange.com/questions/37581/latex-figures-side-by-side
\usepackage{subcaption}
% For footnote in title
% https://tex.stackexchange.com/questions/392232/footnote-on-title-page-in-article/392234#392234
\usepackage[affil-it]{authblk}
\usepackage{blindtext}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{tcolorbox}
\usepackage{import}
%Package for active citation and links within pdf
\usepackage[raiselinks, pdfborder={0 0 0}, bookmarksopen=true, bookmarksnumbered]{hyperref}
\newcommand{\commentfigure}{\scriptsize \textit{Notes: }}

%CAREFUL: needs to be at the end!
\hypersetup{hidelinks,
urlcolor=blue
}

\begin{document}

\title{Coreferences in REL
}



\author{\textsc{Flavio Hafner}
}
\affil{Netherlands eScience Center}

\date{\today}


\clearpage
\maketitle
% ABSTRACT NEEDS WORK

\tableofcontents



\section{Introduction}

REL is a software for entity linking. Entity linking consists of three steps: mention detection, candidate selection, and entity linking. Before entity linking, the current version of REL looks for coreferences in the detected mentions. Consider the example text: "This sentence is about Jimi Hendrix. The music of Hendrix is popular." Here, the second mention "Hendrix" refers to the entity "Jimi Hendrix" in the preceding sentence. Now, for each mention $m_i$, REL checks whether there is another mention $m_j$ that contains mention $m_i$ as a separate word. Denote the set of mentions $m_j$ that $m_i$ refers to by $R_i$.
If $R_i$ is a singleton, REL replaces the original candidate entities of $m_i$ with the candidate entities of $m_j$.
If $R_i$ contains multiple elements, the union of the unique candidate entities for all mentions  $m_j \in R_i$ are assigned as candidate entities for $m_i$. Moreover, for any candidate entity that appears in more than one mention $m_j \in R_i$, their $p(e|m)$ scores are aggregated for the mention $m_i$.

Coreference search can make entity disambiguation more accurate if the original candidates for mention $m_i$ are more noisy than the selected candidates for $m_j$. 
But coreference search also slows down REL for large data sets because it has quadratic time complexity.

I extend REL so that the user can choose from three options on how to deal with coreferences. The first option is "all" which corresponds to the current default of comparing all mentions with all other mentions. The second option is "off" that does not look for coreferences at all. It should be the most efficient. The third option is "lsh" that first applies locality-sensitive hashing (LSH) to all mentions, and then only searches for coreferences among mentions that are clustered together by LSH. If LSH is efficiently implemented, it could strike a balance between efficiency and effectiveness for datasets with many mentions. 

To assess the performance (efficiency and effectiveness) of the options, I run REL with the three options on three data sets: on the AIDA data to check effectiveness, and on synthetic AIDA data---stacking the mentions from AIDA multiple times---to check the time complexity with larger datasets. The purpose of the last run with the msmarco data is to check time complexity on real-world data. I try to check the effectiveness on these data by checking the fraction of mentions that are different between the two approaches. 

In what follows, I define coreferring mentions as mentions like the mention ``Hendrix'' above, and non-coreferring mentions as all other mentions.

The calculations are run on a Dell XPS 13 laptop, using Flair and Wikipedia 2019 for mention detection. I set the limit number of documents from AIDA to 500, which processes all documents. 
For the moment, I hard-code the settings for LSH in REL. The precision-recall tradeoff in LSH means that a higher recall for LSH results in a longer running time of coreference search because, for each mention, \verb|with_coref| checks more mentions. 
The current settings are set to maximize the F-score for LSH on the coreferring mentions in AIDA.



\section{Results}

\subsection{Performance on AIDA data}

Table~\ref{tab:performance_aida_full} shows precision, recall and F1 scores on the AIDA data sets (all documents) with the three options.
The three options do barely differ. The timing results show that ``lsh'' is the slowest option, and this is because setting up LSH has some overhead that dominates the value of reduced coreference search after clustering.

To compare to Erik's results (table 1 in his document), I ran REL also only with 50 documents. The effectiveness results are very similar to his results. The efficiency results, however, differ---in Erik's case, ED takes more than twice as long than in my case. I am not sure why this is.

\begin{table}
 \begin{tabular}{c c c c c c c c c c}
\hline
 coref option & \multicolumn{3}{c}{Mention detection} & \multicolumn{4}{c}{Entity linking}  \\
        & Precision & Recall & F1 & Time & Precision & Recall & F1 \\
 \hline 
 ``all'' & 97.9\% & 62.9\% & 76.6\% & 3.05 sec. & 62.1\% & 39.9\% & 48.6\% \\  
 ``lsh'' & 97.9\% & 62.9\% & 76.6\% & 3.22 sec. & 62.1\% & 39.9\% & 48.6\% \\
 ``off'' & 97.9\% & 62.9\% & 76.6\% & 2.90 sec.  & 62.1\% & 39.9\% & 48.6\% \\ 
\hline 
\end{tabular}
\caption{Performance on AIDA data---all mentions. Setup: processing all documents in AIDA data set with wikipedia 2019.}
\label{tab:performance_aida_full}
\end{table}


Because only a minority of the mentions in the AIDA data are coreferences, I now check the performance for coreferring mentions only. To do this, I classify the gold mentions by whether they are coreferences or not using the same method as in REL, and then compare precision, recall and F1 scores only on these mentions. The results are in table~\ref{tab:performance_aida_corefs}. 

First, using option ``all'' , we can compare the results from the restricted sample with the results from the full sample of mentions. We see that recall for mention detection for coreferring mentions is much worse than for non-coreferring mentions---less than 40 percent of coreferring gold mentions are detected, while 60 percent of all gold mentions are detected. 
This could be (1) because coreferences are harder to detect, or (2) because some detected mentions are not labelled as coreferences because the mention they are referring to is not detected. I think case (1) is more likely.
The tables also indicate that in the ED step, REL has a higher precision but a lower recall for coreferring mentions than non-coreferring mentions. 

Second, comparing effectiveness of ED for the three options, we see that ``lsh'' performs better than ``off'' but worse than ``all''. 


\begin{table}
 \begin{tabular}{c c c c c c c c c c}
\hline
 coref option & \multicolumn{3}{c}{Mention detection} & \multicolumn{4}{c}{Entity linking}  \\
        & Precision & Recall & F1 & Time & Precision & Recall & F1 \\
 \hline 
 ``all'' & 90.7\% & 36.7\% & 52.3\% & --- & 78.8\% & 31.9\% & 45.4\% \\  
 ``lsh'' & 90.7\% & 36.7\% & 52.3\% & --- & 78.8\% & 31.9\% & 45.4\% \\
 ``off'' & 90.7\% & 36.9\% & 52.5\% & --- & 76.8\% & 31.3\% & 44.4\% \\ 
\hline 
\end{tabular}
\caption{Performance on AIDA data---coreferring mentions. Setup: processing all documents in AIDA data set with wikipedia 2019. The timing is not reported because the results are taken from the same run as the results in table~\ref{tab:performance_aida_full}.}
\label{tab:performance_aida_corefs}
\end{table}


\subsection{Time complexity}
To assess time complexity of the different options, I create a synthetic AIDA data set: For each document, I repeat the detected mentions multiple times to increase the size of the input, and then compare the running time for entity linking as a function of the size of the input. Figure~\ref{fig:timing_aida_scale} shows the results.

The panel on the left compares option "off" with option "all". We see that the quadratic time complexity of "all" only starts to impact the overall running time when the number of mentions is higher than around 1500 mentions. This is because the \verb|__predict| function in REL has a relatively large overhead that dominates the running time of coreference search when the number of mentions is small (further evidence is available if necessary). But for an input of 10000 mentions, option ``all'' takes about twice as long as option ``off''. 

The panel on right compares option ``all'' with option ``lsh'' and shows that the time complexity for option ``lsh'' is similar to ``all'', except for large documents where it ``lsh'' is perhaps better than ``all''. To confirm this, I would need to run another test with a larger synthetic data set. 

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/timing_aida_scale.pdf}
  \caption{Time complexity on synthetic AIDA data. Left: comparing ``all'' and ``off''. Right: comparing ``all'' and ``lsh''. The unit of observation is one document stacked $n$ times, 
  $n \in \{5, 50, 100\}$.}
  \label{fig:timing_aida_scale}
\end{figure}

I also profile the execution time of each function in entity disambiguation for the synthetic data. Figure~\ref{fig:profile_aida_scale} shows the the time taken by the function \verb|with_coref|, which is the main function for coreference resolution. The figure suggests a much better time complexity of ``lsh'' compared to ``all'', starting at a document size with around 6000 mentions.

Comparing figures~\ref{fig:timing_aida_scale} and \ref{fig:profile_aida_scale} for the largest document, we see that  switching from ``all'' to ``lsh'' shortens running time by about 15 seconds (figure~\ref{fig:timing_aida_scale}), while figure~\ref{fig:profile_aida_scale} suggests that \verb|with_coref| runs 25 seconds faster. It is unclear what explains this difference. 


\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/profile_aida_scale.pdf}
  \caption{Profiling the function $with\_coref$ with synthetic AIDA data. See the notes for figure~\ref{fig:timing_aida_scale} for details on the sample.}
  \label{fig:profile_aida_scale}
\end{figure}



\subsection{Performance on msmarco data}

\subsubsection{Effectiveness}

To get an idea of recall and precision despite not having a ground truth, I compare the linked entities for mentions with options ``off'' and ``lsh'' to the linked entities with option ``all''. 
During the run with ``all'' I also label mentions as whether they are coreferring or not. This allows me to focus on these mentions in the following analysis.


\paragraph{Approximating recall}
One can compare whether some mentions that are linked to an entity with ``all'' are not linked with the other options. 
The options ``off'' finds 0.36 percent fewer coreferring mentions than ``all''. ``lsh'' finds all mentions that ``all'' also finds. 
This suggests that recall is very similar for all options, and that ``lsh'' is slightly better than ``off''. 


\paragraph{Approximating precision}
Now I focus on the mentions that are linked. I start by comparing the fraction of mentions that have the same predicted entity by aggregating across all documents. Table~\ref{tab:performance_msmarco_avg} shows the results.
Nearly all mentions that are not coreferring have the same linked entity. 
But among coreferring mentions, when using ``off'' only 40 percent of mentions have the same linked entity. 
This implies a substantial variability in the precision of the linked entities for coreferring mentions. But the lack of ground truth prevents from drawing conclusions about whether ``off'' is better than ``all''. 
When using ``lsh'', 61 percent of mentions have the same linked entity.

\begin{table}
 \begin{tabular}{c c c c c c c}
 \hline
    & \multicolumn{3}{c}{``off''} & \multicolumn{3}{c}{``lsh''} \\
   coreferring mention & count & mean & std & count & mean & std \\
 \hline 
 no & 247754 & 0.99 & 0.056 & 247754 &	0.999 &	0.02 \\  
 yes & 8767 & 0.40 & 0.49 & 8767 & 0.90   &	0.30	 \\ 
\hline 
\end{tabular}
\caption{Fraction of mentions for options ``off'' and ``lsh'' with the same linked entity as in option ``all''.}
\label{tab:performance_msmarco_avg}
\end{table}

Now I compare the overlap of predicted entities by document. Overlap is the fraction of mentions in a document that have the same predicted entity. 
Figure~\ref{fig:msmarco_overlap_off} plots the overlap separately for all mentions, coreferring mentions, and non-coreferring mentions. The x-axis is the total number of linked mentions in a document.
All panels show the data for documents with at least five coreferring mentions.

The figure shows considerable variation in the overlap of the predicted entities overall, stemming from variation in the overlap for the coreferring mentions. The data do not allow to draw conclusions about whether there is a relation between the number of linked mentions in the document and the overlap in the predicted entities for coreferring mentions.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/msmarco_overlap_off.pdf}
  \caption{Overlap in predicted entities, by mention type. The unit of observation is the document.}
  \label{fig:msmarco_overlap_off}
\end{figure}


Figure~\ref{fig:msmarco_overlap_lsh} shows the same data, comparing ``lsh'' with ``all''. In line with the aggregate results from table~\ref{tab:performance_msmarco_avg}, ``lsh'' has a higher overlap for coreferring mentions, and perhaps even for non-coreferring mentions.


\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/msmarco_overlap_lsh.pdf}
  \caption{Overlap in predicted entities, by mention type. The unit of observation is the document.}
  \label{fig:msmarco_overlap_lsh}
\end{figure}


\subsubsection{Efficiency}

I now compare the running time for the options. Figure~\ref{fig:timing_msmarco} shows the running time of entity linking as a function of document size; it confirms the findings on the time complexity with the synthetic AIDA data:  the running time for ``off'' and ``all'' is not very different for documents with a few thousand mentions. But for the largest document with 15k mentions, option ``all'' takes about 40 seconds while ``off'' takes 25 seconds.
The figure shows that ``lsh'' has a similar time complexity as ``all'' except for the largest document, where ``lsh'' is faster. 

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/timing_msmarco.pdf}
  \caption{Time complexity on msmarco data for options ``all'' and ``off''. The unit of observation is the document.}
  \label{fig:timing_msmarco}
\end{figure}



%\section{Tuning the LSH parameters}

%The LSH algorithm takes three parameters as inputs that impact the precision and recall of the clustering, which in turn impact recall and running time of the coreference search in ED.
%As of now, these parameters are hard-coded in my development branch of REL. They are chosen heuristically so that they maximize the F1-score among a set of alternative parameter values, but perhaps one can explore more rigorous ways to choose them. The ideal solution would be that they are chosen dynamically, depending on the properties of the data input. I have seen some papers that discuss this, but do not know anything about it. 



\section{Conclusion}

\subsection{Summary}
\begin{enumerate}
 \item For small input data (up to around 2000 mentions), the coreference search is inconsequential for running time. Effectiveness is highest when using option ``all'', and for coreferring mentions ``lsh'' performs equally well on the AIDA test data.
 \item For large input data, efficiency is a problem when doing a full coreference search. Particularly for documents with many coreferring mentions the choice of the option could matter, depending on how strongly coreference search impacts effectiveness in the ED step. 
 \item The results for effectiveness of ED for coreferring mentions perhaps suggest that recall is similar for either ``off'' or ``all''. But the results for precision are not clear-cut. The results from AIDA show similar precision for either option, but the results from msmarco suggest a possibly large impact on precision between options ``off'' and ``all''. 
 The option ``lsh'' is similarly effective as ``all'' but less efficient for the AIDA data; for the msmarco its predictions are much more similar to the option ``all'' than the prediction from option ``off'' are. 
 \item ``lsh'' may also speed up ED for documents with 10k mentions or more, even though there is only one such document in the msmarco data set. For smaller documents, the overhead from ``lsh'' is too large to make a difference on efficiency.  
 %I am nearing the limit for optimizations with my from-scratch solution; external alternatives such as faiss (\href{https://github.com/facebookresearch/faiss}{\underline{link}}) may be faster. 
 %\item For large input data, LSH could be a solution, but precisely in this case a time- and memory-efficient solution is key. Implementing a time-efficient LSH from scratch could take some time. There are external alternatives such as faiss (\href{https://github.com/facebookresearch/faiss}{\underline{link}}).
 \item An important reason for low recall for coreferring mentions is the mention detection, not the disambiguation, but these mentions are a small part of the total number of mentions in the AIDA data. And some of this may be resolved with the update for MD from Erik (which I have not yet integrated in my version of REL).
\end{enumerate}

\subsection{Implications}

\begin{enumerate}
 \item The coreference search option ``all'' limits the scalability of REL for documents with 10k mentions or more.
 \item If the current implementation of coreference search is integrated in the main branch of REL, one can consider an automatic switch to either of the types of coreference search when the number of mentions is large enough, and raise a respective notification to the user. 
\end{enumerate}





%note: the current approach does not solve the quadratic time problem because the one-hot-encoding still loops through all mentions 
%is there an alternative? but in principle the algorithm does have a lower running time, so there should be a way to solve this. the first thing to do is to remove the overhead from the scipy matrix operations. 

\end{document}
