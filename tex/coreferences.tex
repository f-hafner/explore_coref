
\documentclass[a4paper,11pt]{article}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{abstract}
\usepackage{pdfpages} %to include pdfs in the file
\usepackage{booktabs,threeparttable} %for comments to tables
\let\tnote\relax % https://tex.stackexchange.com/questions/51131/using-ctable-package-with-apa6-class?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
\usepackage{geometry}
\hyphenpenalty=6000
%minimizes hyphenation
\usepackage{amsfonts} %for nice mathematical expressions
%\usepackage{acronym}
%package for table of acronyms
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{chngcntr}
%\counterwithout{figure}{section}
\usepackage{lscape}
\usepackage{longtable}
%package for tables of more than 1 page
\usepackage{tabularx}
\usepackage{array}
%packages for fixing rows of tables and centerin their content
\usepackage{rotating}
%for turning text in tables
\usepackage{verbatim}
\usepackage{setspace}
\onehalfspacing
%sets 1.5 times spaces between rows
%alternative: doublespacing
\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
%\usepackage[american]{babel}
%\usepackage{csquotes}
\usepackage{floatrow} %for having 2 graphs side by side
\usepackage{textcomp} % https://tex.stackexchange.com/questions/194083/latex-error-command-textcurrency-unavailable-in-encoding-t1
%standard code for In- and Output of signs
%Careful: american is needed for the APA citation standard
%\bibliography{biblio}
%\usepackage[style=numeric, backend=biber]{biblatex}
%\bibliography{biblio}
%\DeclareLanguageMapping{english}{american-apa}
%\usepackage[style=apa, backend=bibtex]{biblatex}
\usepackage[longnamesfirst,authoryear,sort,comma]{natbib}
\bibliographystyle{ecta}

\usepackage{graphicx}
\usepackage[permil]{overpic}
%Packages for graphics: graphicx for inserting the graphic and overpic to caption it
\usepackage{multirow}
%\usepackage{booktabs}
%Packages for tables: multirow for command \multirow, booktabs for refinements
\usepackage{amsmath, amsthm, amssymb} %Package for formulas, theorems etc.
\newtheorem{prop}{Proposition} % define prop to make propositions. http://tex.stackexchange.com/questions/125981/proposition-command-doesnt-work
  \numberwithin{equation}{section} %for equation numbering according to section
% \usepackage{accents} % for underbar
\usepackage{savefnmark}
%Package for saving footnotes and re-using them
\usepackage{paralist}
%Package for special lists
% To include two graphs in the same line
\usepackage{caption} % https://tex.stackexchange.com/questions/37581/latex-figures-side-by-side
\usepackage{subcaption}
% For footnote in title
% https://tex.stackexchange.com/questions/392232/footnote-on-title-page-in-article/392234#392234
\usepackage[affil-it]{authblk}
\usepackage{blindtext}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{tcolorbox}
\usepackage{import}
%Package for active citation and links within pdf
\usepackage[raiselinks, pdfborder={0 0 0}, bookmarksopen=true, bookmarksnumbered]{hyperref}
\newcommand{\commentfigure}{\scriptsize \textit{Notes: }}

%CAREFUL: needs to be at the end!
\hypersetup{hidelinks,
urlcolor=blue
}

\begin{document}

\title{Coreferences in REL
}



\author{\textsc{Flavio Hafner}
}
\affil{Netherlands eScience Center}

\date{\today}


\clearpage
\maketitle
% ABSTRACT NEEDS WORK

\tableofcontents



\section{Introduction}

REL is a software for entity linking. Entity linking consists of three steps: mention detectio, candidate selection, and entity linking. Before entity linking, the current version of REL looks for coreferences in the detected mentions. Consider the example text: "This sentence is about Jimi Hendrix. The music of Hendrix is popular." Here, the second mention "Hendrix" refers to the entity "Jimi Hendrix" in the preceding sentence. Now, for each mention $m_i$, REL checks whether there is another mention $m_j$ that contains mention $m_i$ as a separate word. Denote the set of mentions $m_j$ that $m_i$ refers to by $R_i$.
If $R_i$ is a singleton, REL replaces the original candidate entities of $m_i$ with the candidate entities of $m_j$.
If $R_i$ contains multiple elements, the union of the unique candidate entities for all mentions  $m_j \in R_i$ are assigned as candidate entities for $m_i$. Moreover, for any candidate entity that appears in more than one mention $m_j \in R_i$, their $p(e|m)$ scores are aggregated for the mention $m_i$.

Coreference search can make entity disambiguation more accurate if the original candidates for mention $m_i$ are more noisy than the selected candidates for $m_j$. 
But coreference search also slows down REL for large data sets because it has quadratic time complexity.

I extend REL so that the user can choose from three options on how to deal with coreferences. The first option is "all" which corresponds to the current default of comparing all mentions with all other mentions. The second option is "off" that does not look for coreferences at all. It should be the most efficient. The third option is "lsh" that first applies locality-sensitive hashing (LSH) to all mentions, and then only searches for coreferences among mentions that are clustered together by LSH. If LSH is efficiently implemented, it could strike a balance between efficiency and effectiveness for datasets with many mentions. 

To assess the performance (efficiency and effectiveness) of the options, I run REL with the three options on three data sets: on the AIDA data to check effectiveness, and on synthetic AIDA data---stacking the mentions from AIDA multiple times---to check the time complexity with larger datasets. The purpose of the last run with the msmarco data is to check time complexity on real-world data. I try to check the effectiveness on these data by checking the fraction of mentions that are different between the two approaches. 

In what follows, I define coreferring mentions as mentions like the mention ``Hendrix'' above, and non-coreferring mentions as all other mentions.

The calculations are run on a Dell XPS 13 laptop, using Flair and Wikipedia 2019 for mention detection. I set the limit number of documents from AIDA to 500, which processes all documents. 
The settings for LSH are such that they have high precision and recall on the coreferring mentions in the AIDA ground truth data. 





\section{Results}

\subsection{Performance on AIDA data}

Table~\ref{tab:performance_aida_full} shows precision, recall and F1 scores on the AIDA data sets (all documents) with the three options. The performance is a little lower for the ``lsh'' option than for the others. Regarding timing for ED, ``off'' is the fastest and ``lsh'' is the slowest option. 
To compare to Erik's results (table 1 in his document), I ran REL also only with 50 documents. The effectiveness results are very similar to his results. The efficiency results, however, differ---in Erik's case, ED takes more than twice as long than in my case. I am not sure why this is.

\begin{table}
 \begin{tabular}{c c c c c c c c c c}
\hline
 coref option & \multicolumn{3}{c}{Mention detection} & \multicolumn{4}{c}{Entity linking}  \\
        & Precision & Recall & F1 & Time & Precision & Recall & F1 \\
 \hline 
 ``all'' & 97.9\% & 62.9\% & 76.6\% & 2.97 sec. & 62.1\% & 39.9\% & 48.6\% \\  
 ``lsh'' & 97.9\% & 62.9\% & 76.6\% & 4.82 sec. & 62.0\% & 39.8\% & 48.5\% \\
 ``off'' & 97.9\% & 62.9\% & 76.6\% & 2.43 sec.  & 62.1\% & 39.9\% & 48.6\% \\ 
\hline 
\end{tabular}
\caption{Performance on AIDA data---all mentions. Setup: processing all documents in AIDA data set with wikipedia 2019.}
\label{tab:performance_aida_full}
\end{table}


Because only a minority of the mentions in the AIDA data are coreferences, I now check the performance for coreferring mentions only. To do this, I classify the gold mentions by whether they are coreferences or not using the same method as in REL, and then compare precision, recall and F1 scores only on these mentions. The results are in table~\ref{tab:performance_aida_corefs}. 

First, we can compare the results from this restricted sample with the results from the full sample of mentions. We see that recall for mention detection for coreferring mentions is much worse than for non-coreferring mentions---only 40 percent of coreferring mentions are detected, while 60 percent of all gold mentions are detected. 
This could be (1) because coreferences are harder to detect, or (2) because some detected mentions are not labelled as coreferences because the mention they are referring to is not detected. I think case (1) is more likely.
The tables also indicate that in the ED step, REL has a higher precision but a lower recall for coreferring mentions than non-coreferring mentions. 

Second, comparing the effectiveness of ED for the three options, we see that ``lsh'' performs worse than either of the alternatives.



\begin{table}
 \begin{tabular}{c c c c c c c c c c}
\hline
 coref option & \multicolumn{3}{c}{Mention detection} & \multicolumn{4}{c}{Entity linking}  \\
        & Precision & Recall & F1 & Time & Precision & Recall & F1 \\
 \hline 
 ``all'' & 90.7\% & 36.7\% & 52.3\% & --- & 78.8\% & 31.9\% & 45.4\% \\  
 ``lsh'' & 90.7\% & 36.7\% & 52.3\% & --- & 76.2\% & 31.0\% & 44.1\% \\
 ``off'' & 90.7\% & 36.7\% & 52.3\% & --- & 76.8\% & 31.3\% & 44.4\% \\ 
\hline 
\end{tabular}
\caption{Performance on AIDA data---coreferring mentions. Setup: processing all documents in AIDA data set with wikipedia 2019. The timing is not reported because the results are taken from the same run as the results in table~\ref{tab:performance_aida_full}.}
\label{tab:performance_aida_corefs}
\end{table}


\subsection{Time complexity}
To assess time complexity of the different options, I create a synthetic AIDA data set: For each document, I repeat the detected mentions multiple times to increase the size of the input, and then compare the running time for entity linking as a function of the size of the input. Figure~\ref{fig:timing_aida_scale} shows the results.

The panel on the left compares option "off" with option "all". We see that the quadratic time complexity of "all" only starts to impact the overall running time when the number of mentions is higher than around 1500 mentions. This is because the \verb|__predict| function in REL has a relatively large overhead that dominates the running time of coreference search when the number of mentions is small (further evidence is available if necessary). But for an input of 10000 mentions, option ``all'' takes twice as long as option ``off''. 

The panel on the right compares option ``all'' with option ``lsh''. We see that LSH currently has worse time complexity than the option ``all''. This is because LSH is implemented inefficiently.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/timing_aida_scale.pdf}
  \caption{Time complexity on synthetic AIDA data. Left: comparing ``all'' and ``off''. Right: comparing ``all'' and ``lsh''. The unit of observation is one document stacked $n$ times, 
  $n \in \{5, 10, 50, 100\}$.}
  \label{fig:timing_aida_scale}
\end{figure}

I also profile the execution time of each function in entity disambiguation for the synthetic data. Figure~\ref{fig:profile_aida_scale} shows the the time taken by the function \verb|with_coref|, which is the main function for coreference resolution. As before, the inefficient implementation of LSH slows down 
\verb|with_coref|. But for the one trial with more than 10000 mentions, the time appears to be better for ``lsh'' than for ``all''.
The current implementation of ``lsh'' creates large time overhead to deal with very many mentions. It is possible that for input with more than 10000 mentions, the quadratic time taken by ``all'' starts to dominate that overhead in ``lsh''. But further tests with larger datasets would be necessary to confirm this possibility.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/profile_aida_scale.pdf}
  \caption{Profiling the function $with\_coref$ with synthetic AIDA data. See the notes for figure~\ref{fig:timing_aida_scale} for details on the sample.}
  \label{fig:profile_aida_scale}
\end{figure}



\subsection{Performance on msmarco data}

\subsubsection{Effectiveness}

To get an idea of recall and precision despite not having a ground truth, I compare the linked entities for mentions with options ``off'' and ``on''.  I exclude the option ``lsh''---it does not run out of memory anymore, but the solution cannot compete on running time with the other options. 
During the run with ``all'' I also label mentions as whether they are coreferring or not. This allows me to focus on these mentions in the following analysis.


\paragraph{Approximating recall}
One can compare whether some mentions that are linked to an entity with ``all'' are not linked with ``off''. I find that only 0.3 percent of coreferring mentions are linked in ``all'' but not linked in ``off'', suggesting that recall is very similar for the two options (but of course it says nothing about the level of recall).


\paragraph{Approximating precision}
Now I focus on the mentions that are linked. I start by comparing the fraction of mentions that have the same predicted entity by aggregating across all documents. Table~\ref{tab:performance_msmarco_avg} shows the results: nearly all mentions that are not coreferring have the same linked entity, but only 40 percent of coreferring mentions have the same linked entity. This implies a huge variability in the precision of the linked entities for coreferring mentions, but the lack of ground truth prevents from drawing conclusions about which of the options is performing better. 

\begin{table}
 \begin{tabular}{c c c c}
 \hline
 coreferring mention & count & mean & std \\
 \hline 
 no & 247754 & 0.99 & 0.056  \\  
 yes & 8767 & 0.402 & 0.490 \\ 
\hline 
\end{tabular}
\caption{Fraction of mentions with the same linked entity for options ``off'' and ``all''.}
\label{tab:performance_msmarco_avg}
\end{table}

Now I compare the overlap of predicted entities by document. Overlap is the fraction of mentions in a document that have the same predicted entity. 
Figure~\ref{fig:msmarco_overlap} plots the overlap separately for all mentions, coreferring mentions, and non-coreferring mentions. The x-axis is the total number of linked mentions in a document.
All panels show the data for documents with at least five coreferring mentions.

The figure shows considerable variation in the overlap of the predicted entities overall, stemming from variation in the overlap for the coreferring mentions. The data do not allow to draw conclusions about whether there is a relation between the number of linked mentions in the document and the overlap in the predicted entities for coreferring mentions.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/msmarco_overlap.pdf}
  \caption{Overlap in predicted entities, by mention type. The unit of observation is the document.}
  \label{fig:msmarco_overlap}
\end{figure}

\subsubsection{Efficiency}

Lastly, figure~\ref{fig:timing_msmarco} show the timing of entity disambiguation with the two options; it confirms the findings on the time complexity with the synthetic AIDA data. Again, the running time is not very different for documents with a few thousand mentions. But for the largest document with 15k mentions, option ``all'' takes about 50 seconds while ``off'' takes 30 seconds.

There is one outlier for the option ``off'' in the top left corner that I do not know where it comes from. 

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.8\textwidth]{../figs/timing_msmarco.pdf}
  \caption{Time complexity on msmarco data for options ``all'' and ``off''. The unit of observation is the document.}
  \label{fig:timing_msmarco}
\end{figure}

\section{Conclusion}

\subsection{Summary}
\begin{enumerate}
 \item For small input data, the coreference search is inconsequential for running time. Effectiveness is highest when using option ``all''.
 \item For large input data, efficiency is a problem when doing a full coreference search. Particularly for documents with many coreferring mentions the choice of the option could matter, depending on how strongly coreference search impacts effectiveness in the ED step. For coreferring mentions, the results from AIDA suggest that coreference search does not matter much. The results from msmarco suggest that coreference search has little impact on recall, but possibly large impact on precision of entity linking of coreferring mentions.
 \item For large input data, LSH could be a solution, but precisely in this case a time- and memory-efficient solution is key. Implementing a time-efficient LSH from scratch could take some time. There are external alternatives such as faiss (\href{https://github.com/facebookresearch/faiss}{\underline{link}}).
 \item An important reason for low recall for coreferring mentions is the mention detection, not the disambiguation, but these mentions are a small part of the total number of mentions in the AIDA data. And some of this may be resolved with the update for MD from Erik (which I have not yet integrated in my version of REL).
\end{enumerate}

\subsection{Implications}

\begin{enumerate}
 \item The coreference search option ``all'' limits the scalability of REL for documents with 10k mentions or more. It is possible to integrate the option ``off'' into REL with little effort. Perhaps one can turn it to ``off'' as a default when the number of mentions is sufficiently large, with a respective warning/notification to the user. 
 \item LSH may be useful for a use case with large documents with possibly many coreferring mentions (but I do not know what kind of documents these could be). The options: (a) external dependency, (b) consult with other people at the eScience Center to find out options to write an efficient solution from scratch, (c) fork from SparseLSH (\href{https://github.com/brandonrobertz/SparseLSH}{\underline{link}}).

\end{enumerate}








%note: the current approach does not solve the quadratic time problem because the one-hot-encoding still loops through all mentions 
%is there an alternative? but in principle the algorithm does have a lower running time, so there should be a way to solve this. the first thing to do is to remove the overhead from the scipy matrix operations. 

\end{document}
